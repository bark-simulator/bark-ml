{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Group 1 in Practical Planning Robust Behavior for Autonomous Driving\n",
    "# Reinforcement Learning using Graph Neural Networks\n",
    "\n",
    "#### Tom Dörr, Marco Oliva, Quoc Trung Nguyen, Silvan Wimmer (Mentor: Patrick Hart)\n",
    "\n",
    "__Objective__: Exploit the graph-like structure of traffic scenarios by applying graph neural networks to the Soft-Actor-Critic algorithm.\n",
    "\n",
    "> ⚠️ **NOTE**  \n",
    "To run this notebook with all dependencies, execute the command `bazel run //docs/gnn_practical_course_2020:run`\n",
    "\n",
    "## Chapter 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import pprint as pp\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import datetime\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "# BARK imports\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "\n",
    "# BARK-ML imports\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "from bark_ml.environments.blueprints import ContinuousMergingBlueprint, ContinuousHighwayBlueprint\n",
    "from bark_ml.environments.single_agent_runtime import SingleAgentRuntime\n",
    "from bark_ml.library_wrappers.lib_tf_agents.agents import BehaviorGraphSACAgent\n",
    "from bark_ml.observers.graph_observer import GraphObserver\n",
    "\n",
    "# Supervised tests\n",
    "from bark_ml.tests.capability_gnn_actor.data_handler import SupervisedData\n",
    "from bark_ml.tests.capability_gnn_actor.actor_nets import ConstantActorNet, RandomActorNet\n",
    "\n",
    "# Report helper functions\n",
    "from helper_functions import configurable_setup, benchmark_actor, explain_observation, clean_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: The GraphObserver\n",
    "To convert the state of the world into a graph, we needed to implement a new observer.\n",
    "In this chapter, we briefly introduce the working mechanisms of the `GraphObserver`.\n",
    "\n",
    "<img src=\"images/observer.png\" width=\"700\">\n",
    "\n",
    "\n",
    "The `GraphObserver` has the following parameters which can be set in with the ParameterServer (i.e. with `params[\"ML\"][\"GraphObserver\"]`):\n",
    "- **AgentLimit**: The maximum number of agents that can be observed. Default is `4`.\n",
    "- **NormalizationEnabled**: Whether node and edge features are normalized.\n",
    "- **VisibilityRadius**: The radius in which an agent can 'see', i.e. detect other agents. Default is `50`.\n",
    "- **SelfLoops**: Whether each node has an edge pointing to itself. Default is `False`\n",
    "- **EnabledNodeFeatures**: The list of available node features, given by their string key that the observer should extract from the world and insert into the observation. For a list of available features, refer to the list returned by `GraphObserver.available_node_attributes()`.\n",
    "- **EnabledEdgeFeatures**: The list of available edge features, given by their string key that the observer should extract from the world and insert into the observation. For a list of available features, refer to the list returned by `GraphObserver.available_edge_attributes()`.\n",
    "\n",
    "There are two main interfaces:\n",
    "\n",
    "- The `Observe(world)` method returns an observation based on the current snapshot of the world by extracting node attributes, an adjacency matrix, and edge attributes. An observation is a 1D `tf.Tensor` concatenating all information of the graph.\n",
    "\n",
    "- The `graph(observations, graph_dims, dense=False)` method basically performs the inverse operation. It accepts a batch of observations (as generated by the `Observe` method) as input and returns a batch of graphs. The argument `dense` specifies the format of the returned graph representation (for further details see the function docstring).\n",
    "\n",
    "Let's look at a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# create an environment that we want to observe\n",
    "params = ParameterServer(filename='data/tfa_gnn_params.json')\n",
    "bp = ContinuousHighwayBlueprint(params, num_scenarios=2, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example explains one exemplary observation and its parts. Further details for the parts can be looked up e.g. in the documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_attributes(flattened matrix of original shape 4x11) (nodes x node attributes):\n",
      " [ 0.4999322  -0.64995706 -0.5        -0.45696357 -0.4999322   1.\n",
      " -0.4999322   0.82497853 -0.49444506  0.64952433 -0.4        -0.4999322\n",
      " -0.66071093 -0.5        -0.4765408  -0.4999322   1.          0.\n",
      "  0.83035547 -0.5         0.6600226  -0.4         0.4999322  -0.7466602\n",
      " -0.5        -0.46313983 -0.4999322   1.         -0.4999322   0.87333006\n",
      " -0.49475256  0.74617344 -0.4        -0.4999322  -0.53071094 -0.5\n",
      " -0.45591465 -0.4999322   1.          0.          0.76535547 -0.5\n",
      "  0.5300765  -0.4       ]\n",
      "Adjacency matrix(flattened matrix of original shape 4x4) (nodes x nodes):\n",
      " [0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
      "Edge_attributes(flattened matrix of original shape 16x4) (number of edges x edge attributes):\n",
      " [ 0.          0.          0.          0.          0.9998644   0.01075391\n",
      "  0.01957722  0.          0.          0.09670313  0.00617626  0.\n",
      "  0.9998644  -0.1192461  -0.00104893  0.         -0.9998644  -0.01075391\n",
      " -0.01957722  0.          0.          0.          0.          0.\n",
      " -0.9998644   0.08594922 -0.01340096  0.          0.         -0.13\n",
      " -0.02062614  0.          0.         -0.09670313 -0.00617626  0.\n",
      "  0.9998644  -0.08594922  0.01340096  0.          0.          0.\n",
      "  0.          0.          0.9998644  -0.21594922 -0.00722519  0.\n",
      " -0.9998644   0.1192461   0.00104893  0.          0.          0.13\n",
      "  0.02062614  0.         -0.9998644   0.21594922  0.00722519  0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# let's initialize the environment and call Observe() (this is happening internally in env.reset)\n",
    "observation = env.reset()\n",
    "\n",
    "explain_observation(observation, observer.graph_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check out what happens when we convert this observation back into a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4, 11), dtype=float32, numpy=\n",
       " array([[[ 0.4999322 , -0.64995706, -0.5       , -0.45696357,\n",
       "          -0.4999322 ,  1.        , -0.4999322 ,  0.82497853,\n",
       "          -0.49444506,  0.64952433, -0.4       ],\n",
       "         [-0.4999322 , -0.66071093, -0.5       , -0.4765408 ,\n",
       "          -0.4999322 ,  1.        ,  0.        ,  0.83035547,\n",
       "          -0.5       ,  0.6600226 , -0.4       ],\n",
       "         [ 0.4999322 , -0.7466602 , -0.5       , -0.46313983,\n",
       "          -0.4999322 ,  1.        , -0.4999322 ,  0.87333006,\n",
       "          -0.49475256,  0.74617344, -0.4       ],\n",
       "         [-0.4999322 , -0.53071094, -0.5       , -0.45591465,\n",
       "          -0.4999322 ,  1.        ,  0.        ,  0.76535547,\n",
       "          -0.5       ,  0.5300765 , -0.4       ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=\n",
       " array([[[0., 1., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 1., 0., 1.],\n",
       "         [1., 1., 1., 0.]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4, 4, 4), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.9998644 ,  0.01075391,  0.01957722,  0.        ],\n",
       "          [ 0.        ,  0.09670313,  0.00617626,  0.        ],\n",
       "          [ 0.9998644 , -0.1192461 , -0.00104893,  0.        ]],\n",
       " \n",
       "         [[-0.9998644 , -0.01075391, -0.01957722,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [-0.9998644 ,  0.08594922, -0.01340096,  0.        ],\n",
       "          [ 0.        , -0.13      , -0.02062614,  0.        ]],\n",
       " \n",
       "         [[ 0.        , -0.09670313, -0.00617626,  0.        ],\n",
       "          [ 0.9998644 , -0.08594922,  0.01340096,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.9998644 , -0.21594922, -0.00722519,  0.        ]],\n",
       " \n",
       "         [[-0.9998644 ,  0.1192461 ,  0.00104893,  0.        ],\n",
       "          [ 0.        ,  0.13      ,  0.02062614,  0.        ],\n",
       "          [-0.9998644 ,  0.21594922,  0.00722519,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ]]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = tf.expand_dims(observation, 0) # add a batch dimension\n",
    "observer.graph(observation, observer.graph_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Graph Neural Networks\n",
    "Before diving into how we apply graph neural networks to our problem, let's have a **very brief** overview about the idea behind them.  \n",
    "Most importantly, they operate on graph structured data, i.e. data consisting of \n",
    "- **Nodes:** feature vectors (node embeddings) of some data entities (and optionally a label), in our case each vehicle is a node\n",
    "- **Edges:** specified links between nodes\n",
    "- **Edge features:** optionally, each link between nodes can have its own feature vector\n",
    "\n",
    "In the section about the `GraphObserver` above, we've already seen how this graph can look like in our scenario. Let's take a step back and use a simplified visualization where the green node represents the ego vehicle and the remaining nodes are other vehicles in its vicinity on the road.\n",
    "\n",
    "![Schematic view of a GNN](images/simple_gnn.png)\n",
    "\n",
    "The ego node is connected to both other nodes (it \"sees\" the other nodes) which in turn do not see each other.\n",
    "\n",
    "Now, the nodes send messages (their current embeddings) along all outgoing links (here, all links are bidrectional), propagated through a neural network. From now on, we refer to this neural network as the _message passing layer(s)_.\n",
    "\n",
    "> **NOTE**  \n",
    "All edges share the same neural network, instead of each edge having its own weights.\n",
    "\n",
    "Each node aggregates all incoming messages using an aggregation function, like summing or averaging. The result is then processed by another neural network, e.g. a recurrent unit, which computes the new embedding of the node.\n",
    "\n",
    "\n",
    "In our project, we have integrated two different libraries that offer GNN implementations:\n",
    "1. [tf2_gnn](https://github.com/microsoft/tf2-gnn): the library that was initially planned to be used in the project\n",
    "2. [Spektral](https://graphneural.network/#installation): a library that supports edge features, which `tf2_gnn` does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: The `GraphNetwork` class\n",
    "\n",
    "As an abstraction over the specific implementation of the graph neural network, we implemented a wrapper class called `GraphNetwork`. Its primary function is to act just as a GNN and so the only interface is the `call` function that accepts a batch of observations (array representations of graphs) and returns a batch of updated node embeddings for each graph.\n",
    "\n",
    "In order to support `tf2_gnn` and `Spektral`, we have two distinct call implementations, one for each library. The `GraphNetwork` class decides which one to call based on the arguments given in the initialization.\n",
    "\n",
    "Both functions however work almost the same:\n",
    "1. Convert the given observations into nodes, edges and, when using `Spektral`, edges features.\n",
    "2. Call the respective library with the converted graph representation.\n",
    "\n",
    "When specifying `Spektral` as the GNN library, the call function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how the embeddings of the ego agent have changed:\n",
      "\n",
      "old embeddings of shape (5,): \n",
      "[0.88748217 0.6305756  0.1297453  0.86862236 0.00165058]\n",
      "\n",
      "new embeddings of shape (16,): \n",
      "[0.08864352 0.         0.         0.45904642 0.6646736  0.\n",
      " 0.         0.11452283 0.         0.03114677 0.15341747 0.43506444\n",
      " 0.4339314  0.84074783 0.28326586 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from spektral.layers import EdgeConditionedConv\n",
    "from helper_functions import get_sample_observations, graph_dims\n",
    "\n",
    "def call_spektral_demo(observations):\n",
    "    # define the layers of the GNN (normally, this happens upon initialization)\n",
    "    \n",
    "    # this defines an edge-conditioned convolution as the message passing layer\n",
    "    # the `kernel_network` argument defines the layers of the edge neural network\n",
    "    edge_convolution = EdgeConditionedConv(channels=16, kernel_network=[128], activation=\"relu\")\n",
    "\n",
    "    def call_spektral(observations, training=False):\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features (embeddings)\n",
    "        # A: binary adjacency matrix specifying edges in the graph\n",
    "        # E: tensor containg edge features\n",
    "        old_embeddings, A, E = GraphObserver.graph(observations, graph_dims)\n",
    "\n",
    "        # pass the inputs through an edge conditioned convolution\n",
    "        # layer and receive new node embeddings\n",
    "        new_embeddings = edge_convolution([old_embeddings, A, E])\n",
    "\n",
    "        # output the final transformed node embeddings\n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_spektral(observations)\n",
    "    \n",
    "    print(\"Here's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# call the function with sample observations\n",
    "call_spektral_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, when `tf2_gnn` is specified, the implementation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's how the embeddings of the ego agent have changed:\n",
      "\n",
      "old embeddings of shape (5,): \n",
      "[0.5538787  0.6449515  0.9806703  0.38632354 0.78949356]\n",
      "\n",
      "new embeddings of shape (16,): \n",
      "[0.01485924 0.         0.02307033 0.         0.08963607 0.\n",
      " 0.03004937 0.         0.04181047 0.         0.06556597 0.\n",
      " 0.         0.         0.01167138 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from tf2_gnn.layers import GNN, GNNInput\n",
    "\n",
    "def call_tf2_gnn_demo(observations):\n",
    "    # the number and types of layers in the GNN are all encoded\n",
    "    # in the parameters dictionary, let's stick to the default for now\n",
    "    gnn_params = GNN.get_default_hyperparameters()\n",
    "\n",
    "    # uncomment the following two lines to have a look at them\n",
    "    # print(f'GNN parameters:')\n",
    "    # pp.pprint(gnn_params)\n",
    "\n",
    "    # initialize a GNN instance which acts as a keras layer\n",
    "    gnn = GNN(gnn_params)\n",
    "\n",
    "    def call_tf2_gnn(observations, training=False):\n",
    "        batch_size = tf.constant(observations.shape[0])\n",
    "\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features\n",
    "        # A: dense adjacency list in the format [[0, 1], [2, 4]]\n",
    "        #    specifying source and target node ids of an egde\n",
    "        # node_to_graph_map: a tensor that assigns each node in X to a graph\n",
    "        old_embeddings, A, node_to_graph_map = GraphObserver.graph(\n",
    "          observations,\n",
    "          graph_dims=graph_dims, \n",
    "          dense=True)\n",
    "        \n",
    "        # build the struct that tf2_gnn expects as input\n",
    "        gnn_input = GNNInput(\n",
    "          node_features=old_embeddings,\n",
    "          adjacency_lists=(A,),\n",
    "          node_to_graph_map=node_to_graph_map,\n",
    "          num_graphs=batch_size,\n",
    "        )\n",
    "\n",
    "        new_embeddings = gnn(gnn_input, training=training)\n",
    "        \n",
    "        # only for demo purposes\n",
    "        old_embeddings = tf.reshape(old_embeddings, [batch_size, graph_dims[0], -1])\n",
    "        new_embeddings = tf.reshape(new_embeddings, [batch_size, 5, -1])\n",
    "        \n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_tf2_gnn(observations)\n",
    "    \n",
    "    print(\"\\nHere's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# call the function with sample observations\n",
    "call_tf2_gnn_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the GNN functionality nicely abstracted behind this wrapper, we can now easily integrate it into the Soft-Actor-Critic framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: The Soft-Actor-Critic Algorithm with Graph Neural Networks\n",
    "\n",
    "Next, let's examine the integrated system.\n",
    "\n",
    "We want to exploit the graph-like structure of traffic scenarios and have already encoded the state of the world as a graph. Now, we want to apply graph neural networks to the SAC algorithm. \n",
    "\n",
    "The resulting actor and critic networks are quite similar in structure. Here's how they work and what they compute.\n",
    "\n",
    "### The Actor Network\n",
    "\n",
    "Implemented in the class `GNNActorNetwork`.\n",
    "\n",
    "\n",
    "**Input**: a batch of observations of shape _(batch_size, observation_size)_  \n",
    "**Output**: a batch of a normal distributions over the action space from which the policy will sample the actions performed by the agent\n",
    "\n",
    "![Actor Network Architecture](images/actor_architecture.png)\n",
    "\n",
    "**1. GNN**  \n",
    "The observations are directly fed into the graph neural network (a `GraphNetwork` instance). It converts the observations into graphs and computes new node embeddings for each graph by means of message passing and aggregation.\n",
    "\n",
    "> **NOTE**  \n",
    "From here on, we're only interested in the embeddings of the ego agent. Hence, instead of feeding the whole graph representation into the encoding network, we extract the embeddings of the first node of each graph, which represents the ego agent.\n",
    "\n",
    "**2. Encoding Network**  \n",
    "In the encoding network, the node embeddings of the ego agent are now passed through a series of dense layers. Depending on the parameters passed into the actor, we can also add convolutions, dropout and other types of layers here.\n",
    "\n",
    "**3. Projection Network**  \n",
    "Finally, the projection network receives the hidden representations after the encoding network and computes a normal distribution over the action space for each observation contained in the batch, modeled by a mean and a standard deviation.\n",
    "\n",
    "In a very simplified manner for brevity, the implementation of the actor's `call` function looks as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "def call(self, observations, training=False):\n",
    "    # get the updated node embeddings\n",
    "    output = self._gnn(observations, training=training)\n",
    "\n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the ego agent's node embeddings through the encoder\n",
    "    output = self._encoder(output, training=training)\n",
    "    \n",
    "    # compute a normal distribution\n",
    "    output = self._projection_net(output, training=training)\n",
    "\n",
    "    return output\n",
    "```\n",
    "\n",
    "### The Critic Network\n",
    "\n",
    "Implemented in the class `GNNCriticNetwork`.\n",
    "\n",
    "**Input**: a batch of observation-action pairs, i.e. `[obs, action]` with shapes _(batch_size, observation_size)_ and _(batch_size, 2)_  \n",
    "**Output**: a scalar value assigned to each observation-action pair\n",
    "\n",
    "\n",
    "The major difference compared to the actor network is that in the critic, we have two parallel pipelines for the observations and their corresponding actions.\n",
    "\n",
    "![Critic Network Architecture](images/critic_architecture.png)\n",
    "\n",
    "**1. Actions**  \n",
    "The actions are simply passed into an action encoding network that works similar to the encoding network of the actor network, i.e. a series of dense layers with optional convolutions, dropout layers, etc.\n",
    "\n",
    "**2. Observations**  \n",
    "The observations are processed in the exact same way as in the actor network. We compute new graph representations in the GNN, extract the ego node embeddings and pass them through an encoding network.\n",
    "\n",
    "**3. Joining Actions and Observations**  \n",
    "After receiving the outputs from the action and observation encoding networks, we concatenate the observation-action pair of each element in the batch to one feature vector.  \n",
    "Finally, we pass this concatenated state through a fully connected joint network which outputs a scalar value for each observation-action pair.\n",
    "\n",
    "Again, a simplified version of the implemenation looks like this:  \n",
    "```python\n",
    "def call(self, inputs, training=False):\n",
    "    observations, actions = inputs\n",
    "     \n",
    "    # get the updated node embeddings\n",
    "    node_embeddings = self._gnn(observations, training=training)\n",
    "    \n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    node_embeddings = node_embeddings[:, 0]\n",
    "    \n",
    "    # pass the node embeddings through their observation encoder\n",
    "    node_embeddings = self._observation_encoder(node_embeddings, trainig=training)\n",
    "    \n",
    "    # do the same for the actions with a different action encoder\n",
    "    actions = self._action_encoder(actions, training=training)\n",
    "    \n",
    "    # concatenate observations and actions into one vector\n",
    "    joint = tf.concat([node_embeddings, actions], 1)\n",
    "    \n",
    "    # compute a scalar output value\n",
    "    output = self._joint_net(joint, training=training)\n",
    "\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Putting It All Together and Setting up an Example\n",
    "\n",
    "Now, let's set up an SAC-agent using the graph neural networks described above to be used in BARK-ML.\n",
    "\n",
    "We start out with the default parameter set as defined in `tfa_sac_gnn_spektral_default.json` and make some optional changes afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParameterServer(filename=\"../../examples/example_params/tfa_sac_gnn_spektral_default.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the GNN-related parameters. We use the same GNN configuration in the actor and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a spektral GNN\n",
    "params['ML']['BehaviorGraphSACAgent']['GNN']['Library'] = 'spektral'\n",
    "    \n",
    "# use two message passing layers with 80 channels of node embeddings each\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"GNN\"][\"NumMpLayers\"] = 2\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"GNN\"][\"MpLayersHiddenDim\"] = 80\n",
    "    \n",
    "# use two fully connected layers in the edge feature mlp of each message passing layer\n",
    "params['ML']['BehaviorGraphSACAgent']['GNN']['EdgeFcLayerParams'] = [128, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure the layers that make up the encoding networks in the actor and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"CriticJointFcLayerParams\"] = [128, 128]\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"CriticObservationFcLayerParams\"] = [128, 128]\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"ActorFcLayerParams\"] = [256, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure the `GraphObserver`.\n",
    "Here we specify that it should always observe at most 4 agents simultaneously, i.e. the ego agent and its three nearest agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"ML\"][\"GraphObserver\"][\"AgentLimit\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify which features the graph observer should use in the node embeddings and the edges. This is useful since not all environments contain the same information and thus, some features might not be possible to compute. To get a list of all available features, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available node features:\n",
      "  'x': The x-components of the agent's position.\n",
      "  'y': The y-components of the agent's position.\n",
      "  'theta': The current heading angle of tha agent.\n",
      "  'vel': The current velocity of the agent.\n",
      "  'goal_x': The x-component of the goal's position.\n",
      "  'goal_y': The y-component of the goal's position.\n",
      "  'goal_dx': The difference in the x-component of the agent's and the goal's position.\n",
      "  'goal_dy': The difference in the y-component of the agent's and the goal's position.\n",
      "  'goal_theta': The goal heading angle.\n",
      "  'goal_d': The euclidian distance of the agent to the goal.\n",
      "  'goal_vel': The goal velocity.\n",
      "\n",
      "Available edge features:\n",
      "  'dx': The difference in the x-position of the two agents.\n",
      "  'dy': The difference in the y-position of the two agents.\n",
      "  'dvel': The difference in the velocity of the two agents.\n",
      "  'dtheta': The difference in the heading angle of the two agents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available node features:\")\n",
    "for key, value in GraphObserver.available_node_attributes(with_descriptions=True).items():\n",
    "    print(f\"  '{key}': {value}\")\n",
    "\n",
    "print(f\"\\nAvailable edge features:\")\n",
    "for key, value in GraphObserver.available_edge_attributes(with_descriptions=True).items():\n",
    "    print(f\"  '{key}': {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we train an agent on the `ContinuousMergingBlueprint` where the goal definition does not contain velocity information, so we can not use the `goal_vel` node feature. So let's configure the `GraphObserver` to use all available node features, except `goal_vel`.\n",
    "\n",
    "In the edges, we want to use all available features, so we don't specify anything. The `GraphObserver` always defaults to using all features if nothing is explicitely configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled_node_features = GraphObserver.available_node_attributes()[:-1]\n",
    "params[\"ML\"][\"GraphObserver\"][\"EnabledNodeFeatures\"] = enabled_node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you feel like experimenting, expand the following dropdown to get an overview of the most important parameters you can tweak.  \n",
    "> **NOTE**  \n",
    "The dropdown is not visible when viewing the notebook on GitHub.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>List of the most important paramaters</b></summary>\n",
    "<br>\n",
    "  <b>Description:</b> Specifies the maximum number of agents that are included in an observation. (int)<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['AgentLimit'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies whether each node in the graph will have an edge pointing to itself. (Bool)<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['SelfLoops'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the features that the GraphObserver will include in the node embeddings. [str]<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['EnabledNodeFeatures'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the features that the GraphObserver will include in the edges. [str]<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['EnabledEdgeFeatures'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the actor encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['ActorFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic action encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticActionFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic observation encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticObservationFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic joint network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticJointFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the number of message passing layers in the GNN. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['NumMpLayers'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the number of units in the message passing layers in the GNN. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MpLayersHiddenDim'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies which library to use as the GNN implementation, either \"tf2_gnn\" or \"spektral\". <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['Library'] <br>\n",
    "  \n",
    "  <h3>The following parameters only apply to TF2-GNN.</h3>\n",
    "\n",
    "  <br>\n",
    "  <b>Description:</b> The identifier of the message passing class to be used, here: a relational gated convolution network. (str)\n",
    "      <br><i>NOTE: when using the 'ggnn' message passing layer, 'MpLayersHiddenDim' must match the number of node features!</i> <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN.message_calculation_class'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> The identifier of the message passing class to be used, here: a gated recurrent unit. (str) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['global_exchange_mode'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies after how many message passing layers a dense layer is inserted. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['dense_every_num_layers'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies after how many message passing layers a global exchange layer is inserted. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN.global_exchange_every_num_layers'] <br>\n",
    "  \n",
    "  <h3>The following parameters only apply to Spektral.</h3>\n",
    "\n",
    "  <b>Description:</b> Specifies the number of channels in the edge conditioned convolution layer. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MPChannels'] <br>\n",
    "  \n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) in the edge network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['EdgeFcLayerParams'] <br>\n",
    "  \n",
    "  <b>Description:</b> Specifies the activation function of the message passing layer. (str) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MPLayerActivation'] <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure the BARK-ML environment and our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# create environment\n",
    "bp = ContinuousMergingBlueprint(params, num_scenarios=1, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)\n",
    "    \n",
    "# create agent\n",
    "agent = BehaviorGraphSACAgent(environment=env, observer=observer, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAGENT SUMMARY\u001b[0m\n",
      "\n",
      "Network                        Parameters\n",
      "==========================================\n",
      "ActorNetwork...................... 547.300\n",
      "CriticNetwork..................... 553.441\n",
      "CriticNetwork2.................... 553.441\n",
      "TargetCriticNetwork1.............. 553.441\n",
      "TargetCriticNetwork2.............. 553.441\n",
      "------------------------------------------\n",
      "Total parameters                 2.761.064\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import prepare_agent, summarize_agent\n",
    "\n",
    "# only for demo purposes\n",
    "prepare_agent(agent, params, env)\n",
    "summarize_agent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Verify the Actor Network with Supervised Learning\n",
    "In this part, we briefly introduce a supervised learning setting which helps to quickly debug different actor implementations. It is evaluated whether the actor network is capable of overfitting a small dataset by comparing it to a `RandomActor` and a `ConstantActor`. The `RandomActor` simply outputs a random label within a prespecified bound. The `ConstantActor` on the other hand always outputs the mean label of the training dataset. This chapter demonstrates what we've implemented in `py_gnn_actor_tests`.\n",
    "\n",
    "Additionally, the performance while learning is compared to the standard SAC actor for better comparison with the help of TensorBoard.\n",
    "\n",
    "But first, let's define a few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames for default parameter files\n",
    "filename_tf2_gnn = \"../../examples/example_params/tfa_sac_gnn_tf2_gnn_default.json\"\n",
    "filename_spektral = \"../../examples/example_params/tfa_sac_gnn_spektral_default.json\"\n",
    "filename_normal = \"../../examples/example_params/tfa_params.json\"\n",
    "\n",
    "params_tf2_gnn = ParameterServer(filename=filename_tf2_gnn)\n",
    "params_spektral = ParameterServer(filename=filename_spektral)\n",
    "params_normal = ParameterServer(filename=filename_normal)\n",
    "\n",
    "num_scenarios = 3\n",
    "log_dir = \"supervised/summary\"\n",
    "data_path = \"supervised/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the different actors for benchmarking and fetch or load a small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# get an observer and an actor net configured with Spektral\n",
    "observer, spektral_actor = configurable_setup(params_spektral, num_scenarios=num_scenarios);\n",
    "\n",
    "# get an actor net configured with tf2_gnn\n",
    "_, tf2_gnn_actor = configurable_setup(params_tf2_gnn, num_scenarios=num_scenarios);\n",
    "\n",
    "# get a normal SAC actor (without GNNs)\n",
    "_, normal_sac_actor = configurable_setup(params_normal, num_scenarios=num_scenarios, graph_sac=False);\n",
    "\n",
    "# get a random actor (outputs random labels with uniform distribution within bounds)\n",
    "random_actor = RandomActorNet(low=[0, -0.4], high=[0.1, 0.4])\n",
    "\n",
    "# construct a supervised dataset using the observer as data generator\n",
    "dataset = SupervisedData(observer, params_tf2_gnn, batch_size=32, train_split=0.8,\n",
    "                         data_path=data_path, num_scenarios=num_scenarios);\n",
    "\n",
    "# get a constant actor (outputs constant mean labels of train_dataset)\n",
    "constant_actor = ConstantActorNet(dataset=dataset._train_dataset)\n",
    "\n",
    "\n",
    "actors = {\n",
    "    \"tf2_gnn_actor\": {\"actor\": tf2_gnn_actor},\n",
    "    \"spektral_actor\": {\"actor\": spektral_actor},\n",
    "    \"normal_sac_actor\": {\"actor\": normal_sac_actor},\n",
    "    \"random_actor\": {\"actor\": random_actor},\n",
    "    \"constant_actor\": {\"actor\": constant_actor}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the magic starts.\n",
    "All trainable actors are trained (the RandomActor and ConstantActor are not trainable) for a few epochs. The results of the training can then be examined in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marco.oliva/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Delete all old logs if some exist\n",
    "clean_log_dir(log_dir)\n",
    "old_logs = []\n",
    "\n",
    "# Run benchmarking\n",
    "for actor_name in actors:\n",
    "    loss = benchmark_actor(actors[actor_name][\"actor\"], dataset, epochs=100, log_dir=log_dir)\n",
    "    actors[actor_name][\"loss\"] = loss\n",
    "    \n",
    "    # Name log clearly with actor_name\n",
    "    # Select correct log\n",
    "    new_logs = os.listdir(log_dir)\n",
    "    log = list(set(new_logs) - set(old_logs))[0]\n",
    "    \n",
    "    # Rename log with actor_name\n",
    "    old_path = os.path.join(log_dir, log)\n",
    "    new_path = os.path.join(log_dir, actor_name)\n",
    "    os.rename(old_path, new_path)\n",
    "    old_logs = os.listdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a0a11d936472f36c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a0a11d936472f36c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir supervised/summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8: Train an actual Agent using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's finally train an agent on a BARK-ML environment.\n",
    "\n",
    "To be able to monitor the training, we launch a TensorBoard instance first. For now, there's nothing to see there, but once we start the training, it will reload and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-959c563ece075671\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-959c563ece075671\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6009;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_log_dir(\"training/sac_gnn_spektral\")\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir training/sac_gnn_spektral/summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a fresh setup of the environment and the agent so that we have it all in one place. Remember that you can customize the agent just as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Set a name for this run to recognize it in TensorBoard.\n",
    "# When training multple times, make sure to set different names, so that the logs are not overwritten.\n",
    "# By default, we use a simple timestamp.\n",
    "training_run_name = str(datetime.datetime.now())\n",
    "\n",
    "# Define how many iterations you want to run (recommended for actual training is >= 40000).\n",
    "num_training_iterations = 200\n",
    "\n",
    "# load default parameters\n",
    "params = ParameterServer(filename='data/tfa_sac_gnn_spektral_default.json')\n",
    "params[\"ML\"][\"BehaviorTFAAgents\"][\"CheckpointPath\"] += f\"/{training_run_name}\"\n",
    "params[\"ML\"][\"TFARunner\"][\"SummaryPath\"] += f\"/{training_run_name}\"\n",
    "params[\"ML\"][\"SACRunner\"][\"NumberOfCollections\"] = num_training_iterations\n",
    "\n",
    "# create environment\n",
    "bp = ContinuousMergingBlueprint(params, num_scenarios=2500, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marco.oliva/Development/bark-ml/python/venv/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import run_rl_example\n",
    "\n",
    "agent = BehaviorGraphSACAgent(environment=env, observer=observer, params=params)\n",
    "\n",
    "# start the example\n",
    "run_rl_example(env, agent, params, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Results\n",
    "\n",
    "### Impact of the Graph Neural Network\n",
    "<img src=\"images/sac_vs_gnn_sac.jpg\" alt=\"\" />\n",
    "The two graphs above show the results of training a standard SAC-agent (blue)  and a GNN-SAC-agent (brown). The top graph shows the mean reward and the bottom one the mean number of steps the agent performed per training iteration.\n",
    "We can recognize three phases the agent goes through during training. \n",
    "\n",
    "\n",
    "#### - Phase 1:\n",
    "\n",
    "The agent learns not to crash. \n",
    "Not crashing results in a reward of 0 at the end of the episode, which is why we can see the mean reward converging towards zero.\n",
    "The GNN-SAC approaches the reward of zero in fewer training iterations than the standard SAC.\n",
    "\n",
    "\n",
    "#### - Phase 2:\n",
    "\n",
    "The agents now have to learn how to actually reach the goal state.\n",
    "To reach the goal, they have to figure out that not crashing is just a local minima and that they therefore can achieve a higher reward by taking some risk and making a lane change into the goal position.\n",
    "The GNN-SAC again needs fewer training iterations to go through this phase, the difference in training iterations is greater than before.\n",
    "\n",
    "\n",
    "#### - Phase 3:\n",
    "\n",
    "In this last phase the agents get more efficient in reaching the goal state.\n",
    "We can see the increase in efficiency by looking at the mean number of steps that each the agent takes per episode. \n",
    "While the agents decrease the number of steps they perform, they keep on increasing the mean reward.\n",
    "As before, the GNN-SAC outperforms the standard SAC when looking at the number of training iterations.\n",
    "\n",
    "\n",
    "However not all runs of the agents are equal.\n",
    "Sometimes the GNN-SAC does not perform better.\n",
    "Additionally, we need to consider the number of features that the agents receive.\n",
    "The standard SAC receives 16 features per observation while the GNN-SAC receives 127 features per observation.\n",
    "Naturally you might wonder if the performance of the GNN-SAC is just due to the larger number of features.\n",
    "The larger number of features is the result of the graph-observer the GNN-SAC uses.\n",
    "To see what impact the graph-observer has, we feed the information from the graph-observer to the standard SAC and again compare it to the GNN-SAC:\n",
    "\n",
    "\n",
    "### Impact of the Graph-Observer\n",
    "<p align=\"center\">\n",
    "<img src=\"images/evaluation_graph_observer.png\" alt=\"\" />\n",
    "</p>\n",
    "\n",
    "The GNN-SAC again needs fewer training iterations, which means that the GNN-SACs performance is not just due to a larger number of features in the observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it actually look? 👀\n",
    "\n",
    "After training the agent for around 40k iterations on the `ContinuousMergingBlueprint`, it has converged to a mean reward of around 1. Its behavior then looks as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/mrcoliva/bark-ml/raw/master/docs/gnn_practical_course_2020/images/gnn-sac-merging-bp.gif\" alt=\"BARK-ML Highway\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10: Future Work\n",
    "\t\n",
    "As we have seen, applying graph neural network to reinforcement learning achieves very promising results. In our project, we used two different libraries: `tf2_gnn` and `spektral`. While `spektral` leverages the use of edge features, `tf2_gnn` does not. It would be interesting to compare the performance of these two library, since edge features would enable the real potential of graph structured data. Another direction is applying graph neural networks to other reinforcement learning algorithms. So far, we only used it with the soft actor critic algorithm. Another good candidate is proximal policy optimization(PPO). It is a good idea to dive deep into the mechanism of graph neural networks and see how it really affects reinforcement learning through these 2 different algorithms. Finally, learning the behavior of an ego agent leads us to the question: Can we use the same model to control the behavior of all other agents? This should be possible since the actor network should generate the same behavior(that maximize the total reward) for each agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendix: Commands "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run the example from the command line with `bazel run //examples:tfa_gnn -- --mode=train`.  \n",
    "The process can be visualized using TensorBoard with `tensorboard --logdir training/sac_gnn_spektral/summaries`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
